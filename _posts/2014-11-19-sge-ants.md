---
layout: post
title: "Ants on the Unacloud cluster using SGE"
description: ""
category: 
tags: []
---
{% include JB/setup %}

## Background

We got the challenge to generate an [Ants](https://github.com/stnava/ANTs) template from 211 images. We had runned ants in the past in a machine containing 32 cores and with 40 images the process took about 8 days. We approximated that in order to calculate this template in the same machine it would take abut a month of processing time. We would have to keep this machine all the time fully dedicated to this task. This was not possible as this machine is usually moved around the lab for different projects. Therefore we decided to try to run Ants in a cluster environment. 

Our university has a cluster platform called [unacloud](https://sistemas.uniandes.edu.co/~unacloud/dokuwiki/doku.php?id=inicio). It uses the computers in several computer rooms by running virtual machines in the background. These virtual machines are copied from an existing image and afterwards its name and ip address are changed. There are several tutorials around the web about installing sge, but here we will summarize the process that worked for us. For more information look at the [bibliography](#biblio) of this post.

We divide the process in the following steps:

1. A *nfs* shared file system, in order to share data and sge configuration.
2. A bind *dns server*, because sge relies a lot on host names.
3. SGE 
4. Ants 
5. Deploying in unacloud


## Test environment

In this example we are going to use a Fedora server, and Debian clients. The master server will be called cangurosge, and the execution nodes will be called canguros1, canguros2, canguros3 ... . We will assume all of them are inside a private network with prefix 10.0.2.

In all machines we have a user named _canguro_. The `/etc/hosts` file contains a line indicating the address of the master, for example


```bash
#/etc/hosts
127.0.0.1        localhost
10.0.2.5         cangurosge
```

In order to avoid problems, and because we assume we are in a trusted private network, we are going to turn off the firewall and selinux in the master. Notice in case you want to deploy this in a public netwowrk you should probably configure this services correctly

```bash
sudo systemctl stop firewalld
sudo systemctl disable firewalld
echo 0 > /sys/fs/selinux/enforce
```

```bash
#/etc/selinux/config
SELINUX=permissive
```

We will configure the master server and an image for a generic client. This image will then be instantiated on each of the hosts.

## NFS

### Server

We need to create a folder for , for example `/home/canguro/nfs` and add it to the exports file. The exports file should look like this

```bash
#/etc/exports
/home/canguro/nfs 10.0.2.1/24(rw,sync,no_all_squash,no_root_squash) 157.253.202.1/24(rw,sync,no_all_squash,no_root_squash)
/usr/share/gridengine/default 10.0.2.1/24(rw,sync,no_all_squash,no_root_squash) 157.253.202.238/24(rw,sync,no_all_squash,no_root_squash)
```

The second line is the folder where the sge configuration will be stored. Afterwards we need to start and enable the necessary services.

```bash
sudo systemctl enable rpcbind
sudo systemctl enable nfs-server
sudo service rpcbind start
sudo service nfs-server start
```

You can find more information about setting up the server at the [fedoraproject wiki][wiki_nfs].

### Client

In order to connect to the server from the client we first need to install the package _nfs-common_	

```bash
apt-get install nfs-common
```

Afterwards we create the directory `/home/canguro/nfs` and mount the remote file system by typing

```bash
mount -t nfs cangurosge:/home/canguro/nfs /home/canguro/nfs
```

You may test this by writing a test file in the master and looking at it in the client or the other way around. For more detailed information look for example at [this guide][debian_nfs]

<a name="biblio"></a>

## DNS Server

SGE is very picky about hostnames and ip addresses therefore it is very convenient to have a working dns server. We are going to create a local _zone_ called `canguro.zone`. For example our server will be accessible through `cangurosge.canguro.zone` and a sample cliente will be accessible through `canguros123.canguro.zone`.

### Server

First we need to install the required packages by typing.

```bash
sudo yum install bind bind-utils
```

Now we will create a key for our clients to update their addresses in the dns. 

```bash
dnssec-keygen -a HMAC-MD5 -b 512 -n USER canguro.zone.
```

This will generate two files with names that look like 

```
Kcanguro.zone.+157+51007.key 
Kcanguro.zone.+157+51007.private
```

We will put this files inside our nfs folder so that the clients can access them. Open the `.private` file and copy the long string after `key`. This string will have to be inserted in the `/etc/named.conf` file. We will need to add several other stuff to this file. Lets look at how it will look at the end.

```bash
#/etc/named.conf
//
// named.conf
//
// Provided by Red Hat bind package to configure the ISC BIND named(8) DNS
// server as a caching only nameserver (as a localhost DNS resolver only).
//
// See /usr/share/doc/bind*/sample/ for example named configuration files.
//

options {
		// Listen on the client ip address
        listen-on port 53 { 127.0.0.1 ; 10.0.2.5; }; 
        listen-on-v6 port 53 { ::1; };
        directory       "/var/named";
        dump-file       "/var/named/data/cache_dump.db";
        statistics-file "/var/named/data/named_stats.txt";
        memstatistics-file "/var/named/data/named_mem_stats.txt";
		// Allow queries only from our private network
        allow-query     { localhost; 10.0.2.1/24; };

        /*
		Comment stripped for brevity
        */
		
        recursion no;

        dnssec-enable yes;
        dnssec-validation yes;
        dnssec-lookaside auto;

        /* Path to ISC DLV key */
        bindkeys-file "/etc/named.iscdlv.key";

        managed-keys-directory "/var/named/dynamic";

        pid-file "/run/named/named.pid";
        session-keyfile "/run/named/session.key";
};

logging {
        channel default_debug {
                file "data/named.run";
                severity dynamic;
        };
};

zone "." IN {
        type hint;
        file "named.ca";
};

key canguro.zone.{
        algorithm "HMAC-MD5";
		// Copy it from the private key file
        secret "";
};

zone "canguro.zone" IN {
        type master;
        file "slaves/canguro.zone";
        allow-update {
                key canguro.zone;
                        };
        notify no;
};

// Inverted ip address from our private network
zone "2.0.10.in-addr.arpa." IN {
        type master;
        file "slaves/canguro.rr.zone";
        allow-update {
                key canguro.zone;
                        };
        notify no;
};

include "/etc/named.rfc1912.zones";
include "/etc/named.root.key";
```

Pay attention to the *allow-query* field, and the *key* block. The `zone "canguro.zone"` block is where we define forward dns, and is further specified in the file "slaves/canguro.zone"; we will create this file in the following step. The `zone "2.0.10.in-addr.arpa."` block is for reverse dns lookup. This block is very tricky and you need to pay special attention to the fact that *ip addresses are inverted*. For example `10.20.30.40` will look like `40.30.20.10`. Now lets look at the `zone` files

```bash
#/var/named/slaves/canguro.zone
$ORIGIN .
$TTL 7200       ; 2 hours
canguro.zone            IN SOA  ns0.canguro.zone. hostmaster.canguro.zone. (
                                2000640    ; serial
                                28800      ; refresh (8 hours)
                                1800       ; retry (30 minutes)
                                604800     ; expire (1 week)
                                86400      ; minimum (1 day)
                                )
                        NS      ns0.canguro.zone.
$ORIGIN canguro.zone.
$TTL 14000      ; 3 hours 53 minutes 20 seconds
ns0                     A       10.0.2.5
```

```bash
#/var/named/slaves/canguro.rr.zone
$ORIGIN .
$TTL 7200       ; 2 hours
2.0.10.in-addr.arpa IN SOA ns0.canguro.zone. hostmaster.canguro.zone. (
                                2000027    ; serial
                                28800      ; refresh (8 hours)
                                1800       ; retry (30 minutes)
                                604800     ; expire (1 week)
                                86400      ; minimum (1 day)
                                )
                        NS      ns0.canguro.zone.
                        PTR     ns0.canguro.zone.
$ORIGIN 2.0.10.in-addr.arpa.
```

Both of these files will get larger as clients begin to add their information. Now finally we need to start the service

```bash
systemctl start named.service
systemctl enable named.service
```

If something fails, run `systemctl status named` to look for a more useful error message. More detailed information can be found [here][bind_guide].

### Client

Now we are going to configure the clients to use our dns server. The easiest way of doing this is by changing the `/etc/resolv.conf` file, however notices this file will be overwritten by the dhcp client. In unacloud ip addresses are static so we will not have this problem.


#/etc/resolv.conf
```bash
domain canguro.zone
search canguro.zone
nameserver 10.0.2.5
nameserver 8.8.8.8
```

The first two lines let us do `ping cangurosge` instead of `ping cangurosge.canguro.zone`. The first nameserver is our own, but this server only resolves names inside canguro.zone. We use a second nameserver (in this example google) so that we can resolve standard domain names. More information about setting up the network in debian can be found [here][debian_dns].

Finally we need to register the clients with the nameserver. For that we need to `cd` into the directory containing the keys and use the ``nsupdate` application. We can do this by running the following script.

```bash
#update_dns.sh

#!/bin/bash
cd /home/canguro/nfs/keys
MYIP=$(hostname -I)
REVERSIP=$(echo $MYIP |awk -F"." '{for(i=NF;i>0;i--) printf i!=1?$i".":"%s",$i}')
MYNAME=$(hostname)
nsupdate -k Kcanguro.zone.+157+51007.private << EOF
server 10.0.2.5
update delete $MYNAME.canguro.zone A
update add $MYNAME.canguro.zone 14000 A $MYIP
send
update add $REVERSIP.in-addr.arpa 14000 PTR $MYNAME.canguro.zone
send
EOF
```

## Grid Engine

Grid Engine is a very complex piece of software, and includes several advanced features like accounting, reservations, quotas and interactive jobs. Fortunately here we only need some basic functionality. 

### Server

In fedora the first step is installing the following packages

```bash
sudo yum install gridengine gridengine-qmaster gridengine-execd gridengine-qmon gridengine-devel
```

This will download the necessary files and create the required users and groups. To complete the installation 

```bash
cd /usr/share/gridengine
./inst_sge -m
# we can accept most defaults except:
# say no to verify permissions 
# choose CLASSIC SPOOLING, not Berkeley db
# you may leave the execution hosts list empty for now
# no shadow host
```

I am not really sure about this but several posts([1][sge_troubleshooting],[2][fedora_sge1],[3][sge_softpanorama]) advise against using berkeley db. Now try running `sge_qmaster`. By default this command launches a daemon, so it should return almost immediately. You can check if it worked through `ps -A | grep sge`. The exec is located at `/usr/share/gridengine/bin/linux-x64`. If you run into problems, you can do the following in order to get a better idea of what the problem is:

```bash
source  /usr/share/gridengine/util/dl.sh 
# adds the dl command to your environment
dl 10
#sets debug level to 10, and turns off daemon mode
sge_execd
```

In my case the sevice file had a wrong location for the PIDFile and so it was not possible to launch the qmaster through systemd. It is ok running it from the command line, but if you want to use systemd here is the file:

```bash
# /usr/lib/systemd/system/sgemaster.service

[Unit]
Description=Gridengine master daemon and scheduler
After=network-online.target

[Service]
Type=forking
EnvironmentFile=-/etc/sysconfig/gridengine
ExecStart=/usr/bin/sge_qmaster
# If running under a different cell, copy this file to /etc/systemd/system/sgemaster.service
# and change default below to the cell name
PIDFile=/var/spool/gridengine/default/spool/qmaster/qmaster.pid

[Install]
WantedBy=multi-user.target

```

Now you can do 

```bash
systemctl start sgemaster
systemctl enable sgemaster
```

It is also a good idea to add the directory `/usr/share/gridengine/default` to the NFS exports list so that the clients can get the configuration from here. 

To test everything is correct you can launch the qmon interface. It looks outdated but does the job. If you get the following screen then everything is good (requires an X session). If you prefer it is also possible to configure and check the status of the gridengine from the command line using commands like [qstat](http://gridscheduler.sourceforge.net/htmlman/htmlman1/qstat.html) and [qconf](http://gridscheduler.sourceforge.net/htmlman/htmlman5/sge_conf.html).

![qmon start screen](/imgs/qmon_start.PNG)

Two final tasks to finish the configuration of the server.

1. Create a hostgroup: click on the "Host" button in qmon, go to the Host Groups Tab and click add. in the top field write @allhosts and click ok. (For the moment there will not be any members)
![Adding a hostgroup through qmon](/imgs/qmon_hostgroup.png)

2. Create a Queue: click on the "Queue Control" button in qmon, in the "Cluster Queues" tab click "Add", write a name for the queue and on the New Host/Hostgroup field write "@allhosts" and cilck on the arrow. You may use the bottom panel to  configure the queue. An important field is "slots", this is the number of processes that will be run simultaneously on each host.

![Adding a queue through qmon](/imgs/qmon_add_queue.png)

### Client

The first step is also to start the debian packages:

```bash
apt-get install gridengine-client gridengine-exec
```

Remember the name of the master is _cangurosge_ and the cell is named _default_ (the default in the `inst_sge` script. You will likely get an error about not finding the qmaster, there are some additional steps in the configuration.

1. Mount the shared "cell" directory

```bash
mount -t nfs cangurosge:/usr/share/gridengine/default /usr/share/gridengine/default
# to keep logs in a common directory
ln -s /usr/share/gridengine/default /usr/share/gridengine/default
```

2. Add as an administrative host, and add to the _allhosts_ group
```bash
SLAVE_NAME = $(hostname)
ssh cangurosge "qconf -ah $SLAVE_NAME"
ssh cangurosge "qconf -aattr hostgroup hostlist $SLAVE_NAME @allhosts"
```

Now you should be ready to run `sge_execd`. This is also a daemon, if there are still problems use `ld.sh` as above. Note that this file is not included in the debian package, but you may copy it from the master.

If everything works you should be able to see the slave in the "Queue Instances" tab of the "Queue Control" panel in Qmon. You can test it by creating a single job which writes the hostname to a file, for example

```bash
#test_job.sh
sleep 60
date >> /home/canguro/nfs/test_job_out.txt
hostname >> /home/canguro/nfs/test_job_out.txt
```

You can then submit this job to the cluster by typing `qsub test_job.sh` in the master. 

## Installing ANTs

The best way to install ants is by compiling it from source. This has to be done separately in the server and the client. The source and instructions for compiling can be found in their [github repository][ants_git]. After compiling copy all the binaries and scripts to a directory called ants_bin otuside _nfs_. This directory should be the same in both master and slave, for example `/home/canguro/ants_bin`

## Deploying in Unacloud

### Server

The server is instantiated in a dedicated machine. The only required step is to add the ip ranges for the clients to:

- The _nfs_ exports file
- The named.conf allow-query field
- Create a zone and named.conf section for reverse ip lookup in this new range

### Clients

For the clients we need to mount the shared file systems, register to the dns, and launch the sge_execd daemon at startup, but after unacloud has finished configuring the network and assigning a name to the client. For this purpose we can crate the following script inside `/etc/init.d`.

```bash
# /etc/init.d/setup_slave.sh

#!/bin/bash

#Wait for unacloud to work
sleep 10

## mount shared folders
mount -t nfs cangurosge:/home/canguro/nfs /home/canguro/nfs
mount -t nfs cangurosge:/usr/share/gridengine/default /usr/share/gridengine/default
sleep 5

## update my data in dns
cd /home/canguro/nfs/keys
MYIP=$(hostname -I)
REVERSIP=$(echo $MYIP |awk -F"." '{for(i=NF;i>0;i--) printf i!=1?$i".":"%s",$i}')
MYNAME=$(hostname)
nsupdate -k Kcanguro.zone.+157+51007.private << EOF
server 10.0.2.5
update delete $MYNAME.canguro.zone A
update add $MYNAME.canguro.zone 14000 A $MYIP
send
update add $REVERSIP.in-addr.arpa 14000 PTR $MYNAME.canguro.zone
send
EOF

sleep 20
## start sge_execd
sge_execd
```

Note that for the _sge_qmaster_ to accept the client it is necessary to first add it to the @allhosts group and register it as an administrative host. The names in Unacloud start with a base name, and are followed by the last two octets of the ip without the dot, for example _canguros202128_ will have an ip that ends in 202.108 . It is convenient to use a script from an existent host that registers all the possible slave names in the master. The master will check that the ip and hostname match, and so we need to fake our identity with the dns server. We can do this using the following script.

```bash
#register_slave2.sh

#!/bin/bash

register_dns() {
## update my data in dns
cd /home/canguro/nfs/keys
MYIP=$(hostname -I)
REVERSIP=$(echo $MYIP |awk -F"." '{for(i=NF;i>0;i--) printf i!=1?$i".":"%s",$i}')
MYNAME=$(hostname)
nsupdate -k Kcanguro.zone.+157+51007.private << EOF
server 157.253.236.165
update delete $MYNAME.canguro.zone A
update add $MYNAME.canguro.zone 14000 A $MYIP
send
update add $REVERSIP.in-addr.arpa 14000 PTR $MYNAME.canguro.zone
send
EOF
}

main() {
if [ $# -lt 1 ]; then
exit 1
fi
local SLAVE_NAME=$1
#set machine name
hostname $SLAVE_NAME
register_dns
echo "sleeping a while"
sleep 1
#register as administrative host
ssh cangurosge "qconf -ah $SLAVE_NAME"
#add to allhosts group
ssh cangurosge "qconf -aattr hostgroup hostlist $SLAVE_NAME @allhosts"
}

main $1
```

And in the terminal

```bash
for i in `seq 2 255`
do echo bash register_slave2.sh canguros239$i
bash register_slave2.sh canguros239$i
sleep 15
done
```

Loading images into unacloud and deploying them is carried out through a web interface which can be accessed at [http://unacloud.uniandes.edu.co/Unacloud2/](http://unacloud.uniandes.edu.co/Unacloud2/). 

## Running ANTs

Now ANTs should be ready to run in the cluster. For example to create a template place all the images in a folder in nfs and run the following command

```bash
export PATH=/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/canguro/.local/bin:/home/canguro/bin:/home/canguro/ants_bin
ANTsPath=/home/canguro/ants_bin/
inputPath=${PWD}/


${ANTsPath}/antsMultivariateTemplateConstruction2.sh \
  -d 3 \
  -o ${inputPath}kmcT_ \
  -i 4 \
  -g 0.25 \
  -c 1 \
  -k 1 \
  -w 1 \
  -f 8x4x2x1 \
  -s 3x2x1x0 \
  -q 100x70x50x10 \
  -n 1 \
  -r 1 \
  -l 1 \
  -m CC[4] \
  -t SyN[0.1,3,0] \
  ${inputPath}/T1_[0-9]*.nii.gz
```

The key parameter is `-c 1` , which tells ants to submit jobs using qsub. Remember to run this command using _screen_ or _nohup_, because it will take a long time and you will eventually want to disconnect from the server.

Open the _antsMultivariateTemplateConstruction2.sh_ script and look for a line that says `QSUBOPTS="" # EDIT THIS
` and change it to

```bash
QSUBOPTS="-r yes" # EDIT THIS
```

## Bibliography

- [Unacloud](unacloud.uniandes.edu.co)
- [Administration Guide Draft/NFS][wiki_nfs]
- [Basic NFS Configuration on Debian 7][debian_nfs]
- [OpenShift Deployment Notes: Running a Local Dynamic DNS Service][bind_guide]
- [Debian NetworkConfiguration][debian_dns]
- [Deploying Sun Grid Engine on a Cluster][fedora_sge1]
- [Grid Engine 6 Troubleshooting][sge_troubleshooting]
- [Installation of Oracle SGE Engine Master Host on Red Hat][sge_softpanorama]
- [Advanced Normalization Tools (ANTs)][ants_git]

[wiki_nfs]: http://fedoraproject.org/wiki/Administration_Guide_Draft/NFS
[debian_nfs]: https://www.linode.com/docs/networking/basic-nfs-configuration-on-debian-7
[bind_guide]: http://openshift.github.io/documentation/oo_notes_running_a_local_ddns.html
[debian_dns]: https://wiki.debian.org/NetworkConfiguration#Defining_the_.28DNS.29_Nameservers
[fedora_sge1]: http://idolinux.blogspot.com/2008/09/deploying-sun-grid-engine-on-cluster.html
[sge_troubleshooting]: http://www.bioteam.net/wp-content/uploads/2009/09/07-SGE-6-Admin-Troubleshooting.pdf
[sge_softpanorama]:http://www.softpanorama.org/HPC/Grid_engine/Implementations/Oracle/installation_of_oracle_sge_master_host.shtml
[ants_git]:https://github.com/stnava/ANTs